/* -*- mode: go; -*-
 *
 * Copyright (C) 2019 Philipp Benner
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

package saga

/* -------------------------------------------------------------------------- */

#include "../../macros.h"

/* -------------------------------------------------------------------------- */

import   "fmt"
import   "math"
import   "math/rand"

import . "github.com/pbenner/autodiff"

/* -------------------------------------------------------------------------- */

#define OBJECTIVE STR_CONCAT(Objective, SAGA_TYPE)

#define GRADIENT_TYPE STR_CONCAT(Gradient, SAGA_TYPE)
#define IN_SITU_TYPE  STR_CONCAT(InSitu,   SAGA_TYPE)

#define L1REG STR_CONCAT(l1regularization, SAGA_TYPE)
#define L2REG STR_CONCAT(l2regularization, SAGA_TYPE)

/* -------------------------------------------------------------------------- */

type OBJECTIVE func(int, VECTOR_TYPE) (CONST_SCALAR_TYPE, VECTOR_TYPE, CONST_SCALAR_TYPE, error)

type IN_SITU_TYPE struct {
  T1 VECTOR_TYPE
  T2 SCALAR_TYPE
}

/* -------------------------------------------------------------------------- */

type GRADIENT_TYPE struct {
  g       VECTOR_TYPE
  w CONST_SCALAR_TYPE
}

func (obj GRADIENT_TYPE) add(v VECTOR_TYPE) {
  for it := v.JointIterator(obj.g); it.Ok(); it.Next() {
    s_a, s_b := it.Get()
    if s_a == nil {
      s_a = v.At(it.Index())
    }
    s_a.SetValue(s_a.GetValue() + obj.w.GetValue()*s_b.GetValue())
  }
}

func (obj GRADIENT_TYPE) sub(v VECTOR_TYPE) {
  for it := v.JointIterator(obj.g); it.Ok(); it.Next() {
    s_a, s_b := it.Get()
    if s_a == nil {
      s_a = v.At(it.Index())
    }
    s_a.SetValue(s_a.GetValue() - obj.w.GetValue()*s_b.GetValue())
  }
}

func (obj *GRADIENT_TYPE) set(g VECTOR_TYPE, w CONST_SCALAR_TYPE) {
  if obj.g == nil {
    obj.g = NULL_VECTOR(g.Dim())
  }
  obj.g.Set(g)
  obj.w = w
}

/* -------------------------------------------------------------------------- */

func L1REG(x Vector, w VECTOR_TYPE, t SCALAR_TYPE, lambda float64) {
  for i := 0; i < x.Dim(); i++ {
    if yi := w.ValueAt(i); yi < 0.0 {
      x.At(i).SetValue(-1.0*math.Max(math.Abs(yi) - lambda, 0.0))
    } else {
      x.At(i).SetValue( 1.0*math.Max(math.Abs(yi) - lambda, 0.0))
    }
  }
}

func L2REG(x Vector, w VECTOR_TYPE, t SCALAR_TYPE, lambda float64) {
  t.Vnorm(w)
  t.Div(CONST_SCALAR_TYPE(lambda), t)
  t.Sub(CONST_SCALAR_TYPE(1.0), t)
  t.Max(CONST_SCALAR_TYPE(0.0), t)
  x.VmulS(w, t)
}

/* -------------------------------------------------------------------------- */

func STR_CONCAT(saga, SAGA_TYPE)(
  f OBJECTIVE,
  n int,
  x Vector,
  gamma Gamma,
  epsilon Epsilon,
  maxEpochs MaxEpochs,
  maxIterations MaxIterations,
  l1reg L1Regularization,
  l2reg L2Regularization,
  hook Hook,
  inSitu *IN_SITU_TYPE) (Vector, error) {

  x1 := AS_VECTOR(x)
  x2 := AS_VECTOR(x)

  // length of gradient
  d := x.Dim()
  // gradient
  var y  CONST_SCALAR_TYPE
  var g1 GRADIENT_TYPE
  var g2 GRADIENT_TYPE

  // allocate temporary memory
  if inSitu.T1 == nil {
    inSitu.T1 = NULL_VECTOR(d)
  }
  if inSitu.T2 == nil {
    inSitu.T2 = NULL_SCALAR()
  }
  // temporary variables
  t1 := inSitu.T1
  t2 := inSitu.T2
  // some constants
  t_n := NEW_SCALAR(float64(n))
  t_g := NEW_SCALAR(gamma.Value)

  // sum of gradients
  s    := NULL_VECTOR(d)
  dict := make([]GRADIENT_TYPE, n)
  // initialize s and d
  for i := 0; i < n; i++ {
    if _, g, w, err := f(i, x1); err != nil {
      return nil, err
    } else {
      dict[i].set(g, w)
      dict[i].add(s)
    }
  }
  y = CONST_SCALAR_TYPE(math.NaN())

  for i_ := 0; i_ < maxIterations.Value && i_/n < maxEpochs.Value; i_++ {
    // execute hook if available
    if hook.Value != nil && hook.Value(x1, s, y) {
      break
    }
    j := rand.Intn(n)

    // get old gradient
    g1 = dict[j]
    // evaluate objective function
    if y_, g, w, err := f(j, x1); err != nil {
      return x1, err
    } else {
      y = y_
      g2.set(g, w)
    }

    t1.VDIVS(s , t_n)
    g2.add(t1)
    g1.sub(t1)
    t1.VMULS(t1, t_g)

    switch {
    case l1reg.Value != 0.0:
      t1.VSUBV(x1, t1)
      L1REG(x2, t1, t2, gamma.Value*l1reg.Value)
    case l2reg.Value != 0.0:
      t1.VSUBV(x1, t1)
      L2REG(x2, t1, t2, gamma.Value*l2reg.Value)
    default:
      x2.VsubV(x1, t1)
    }
    // evaluate stopping criterion
    max_x     := 0.0
    max_delta := 0.0
    for it := x1.JOINT_ITERATOR(x2); it.Ok(); it.Next() {
      s1, s2 := it.GET()
      v1, v2 := 0.0, s2.GetValue()
      if s1 != nil {
        v1 = s1.GetValue()
      }
      if math.IsNaN(v2) {
        return x1, fmt.Errorf("NaN value detected")
      }
      max_x     = math.Max(max_x    , math.Abs(v2))
      max_delta = math.Max(max_delta, math.Abs(v2 - v1))
    }
    if max_x != 0.0 && max_delta/max_x <= epsilon.Value*gamma.Value ||
      (max_x == 0.0 && max_delta == 0.0) {
      return x2, nil
    }
    x1, x2 = x2, x1
    // update table
    g1.sub(s)
    g2.add(s)

    // update dictionary
    dict[j].set(g2.g, g2.w)
  }
  return x1, nil
}
